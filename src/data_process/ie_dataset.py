import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from typing import List, Dict, Tuple, Optional

try:
    from data_process.loader import HateOriDataset
    from data_process.preprocessor import GENERIC_IOB_LABEL_TO_ID, GENERIC_ID_TO_IOB_LABEL, IGNORE_INDEX
except ImportError:
    try:
        from .loader import HateOriDataset
        from .preprocessor import GENERIC_IOB_LABEL_TO_ID, GENERIC_ID_TO_IOB_LABEL, IGNORE_INDEX
    except ImportError:
        print("CRITICAL ERROR: Could not import HateOriDataset or GENERIC IOB label definitions from preprocessor.")
        print("Please ensure loader.py and preprocessor.py are in the correct path and contain GENERIC_IOB definitions.")
        class HateOriDataset: pass # Fallback
        GENERIC_IOB_LABEL_TO_ID = {"O": 0, "B": 1, "I": 2} # Fallback
        GENERIC_ID_TO_IOB_LABEL = {v: k for k, v in GENERIC_IOB_LABEL_TO_ID.items()} # Fallback
        IGNORE_INDEX = -100 # Fallback


class HateIEDataset(Dataset):
    def __init__(self,
                 original_json_path: str,
                 tokenizer_name_or_path: str,
                 max_seq_length: int = 512,
                 # max_char_overlap_ratio_with_other_ta_pairs (å·²ç§»é™¤)
                 ):
        self.original_dataset = HateOriDataset(original_json_path, is_pred=False)
        if not hasattr(self.original_dataset, '__getitem__') or not hasattr(self.original_dataset, '__len__'):
             raise ImportError("Failed to properly load HateOriDataset. Check loader.py.")

        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
        self.max_seq_length = max_seq_length
        
        self.iob_label_to_id = GENERIC_IOB_LABEL_TO_ID
        self.id_to_iob_label = GENERIC_ID_TO_IOB_LABEL

        self.processed_data = self._preprocess_data()
        if not self.processed_data:
            print(f"è­¦å‘Š: _preprocess_data for {original_json_path} returned no samples. "
                  "Check data or preprocessing logic.")

    def _find_first_occurrence(self, text: str, subtext: str) -> Optional[Tuple[int, int]]:
        """æŸ¥æ‰¾subtextåœ¨textä¸­ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®ã€‚"""
        if not subtext or subtext == "NULL":
            return None
        
        pos = text.find(subtext)
        if pos != -1:
            return (pos, pos + len(subtext))
        return None

    def _map_char_span_to_token_span_robust(self,
                                            char_start: int,
                                            char_end: int,
                                            offset_mapping: List[Tuple[int, int]]
                                            ) -> Tuple[int, int]:
        """
        æ›´é²æ£’åœ°å°†å­—ç¬¦spanæ˜ å°„åˆ°token spanã€‚
        è¿”å› (token_start_index, token_end_index) æˆ– (-1, -1) å¦‚æœå¤±è´¥ã€‚
        """
        token_start, token_end = -1, -1
        for i, (offset_s, offset_e) in enumerate(offset_mapping):
            if offset_s == offset_e == 0: continue
            if token_start == -1 and offset_s <= char_start < offset_e:
                token_start = i
            if token_start != -1 and offset_s < char_end and offset_e >= char_start:
                token_end = i
        if token_start != -1 and token_end == -1:
             s_offset, e_offset = offset_mapping[token_start]
             if s_offset <= char_start and e_offset >= char_end: token_end = token_start
        if token_start != -1 and token_end != -1 and token_start <= token_end:
            actual_char_s = offset_mapping[token_start][0]
            actual_char_e = offset_mapping[token_end][1]
            if actual_char_s <= char_start and actual_char_e >= char_end:
                return token_start, token_end
        return -1, -1

    def _apply_iob_labels_to_sequence(self,
                                      token_start_idx: int,
                                      token_end_idx: int,
                                      labels_sequence: List[int]
                                     ):
        """å°†Bå’ŒIæ ‡ç­¾åº”ç”¨åˆ°ç»™å®šçš„labels_sequenceçš„æŒ‡å®štoken spanä¸Šã€‚"""
        if token_start_idx != -1 and token_end_idx != -1 and token_start_idx <= token_end_idx:
            b_id = self.iob_label_to_id["B"]
            i_id = self.iob_label_to_id["I"]
            if labels_sequence[token_start_idx] != IGNORE_INDEX:
                labels_sequence[token_start_idx] = b_id
            for k in range(token_start_idx + 1, token_end_idx + 1):
                if labels_sequence[k] != IGNORE_INDEX:
                    labels_sequence[k] = i_id

    def _preprocess_data(self) -> List[Dict]:
        processed_samples = []
        for i in range(len(self.original_dataset)):
            item_id, content, ta_pairs = self.original_dataset[i]

            if not ta_pairs and not self.original_dataset.is_pred:
                continue

            tokenized_output = self.tokenizer(
                content, max_length=self.max_seq_length, padding="max_length",
                truncation=True, return_offsets_mapping=True,
                return_attention_mask=True, return_token_type_ids=True
            )
            input_ids = tokenized_output["input_ids"]
            attention_mask = tokenized_output["attention_mask"]
            token_type_ids = tokenized_output.get("token_type_ids")
            offset_mapping = tokenized_output["offset_mapping"]
            
            labels_target = [self.iob_label_to_id["O"]] * len(input_ids)
            labels_argument = [self.iob_label_to_id["O"]] * len(input_ids)

            for token_idx, offset in enumerate(offset_mapping):
                if offset == (0, 0) or attention_mask[token_idx] == 0:
                    labels_target[token_idx] = IGNORE_INDEX
                    labels_argument[token_idx] = IGNORE_INDEX
            
            # ä¸å†ä½¿ç”¨å…¨å±€å­—ç¬¦å ç”¨è·Ÿè¸ªå™¨è¿›è¡Œé¢„è¿‡æ»¤

            for ta_pair_idx, ta_pair in enumerate(ta_pairs):
                target_text = ta_pair["target"]
                argument_text = ta_pair["argument"]
                
                # --- 1. æ ‡è®°Targetå±‚ ---
                target_labeled_this_ta_pair = False
                if target_text and target_text != "NULL":
                    # æŸ¥æ‰¾Targetçš„ç¬¬ä¸€ä¸ªå‡ºç°
                    chosen_t_char_span = self._find_first_occurrence(content, target_text)
                    if chosen_t_char_span:
                        t_tok_start, t_tok_end = self._map_char_span_to_token_span_robust(
                            chosen_t_char_span[0], chosen_t_char_span[1], offset_mapping
                        )
                        if t_tok_start != -1:
                            # å¯¹äºä¸¤å±‚æ ‡ç­¾ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨å¯¹åº”å±‚æ‰“æ ‡ï¼Œä¸æ£€æŸ¥æ˜¯å¦å·²è¢«å¦ä¸€å±‚å ç”¨
                            # å› ä¸ºæ¨¡å‹æœ‰ä¸¤ä¸ªç‹¬ç«‹çš„å¤´æ¥é¢„æµ‹è¿™ä¸¤å±‚
                            self._apply_iob_labels_to_sequence(
                                t_tok_start, t_tok_end, labels_target
                            )
                            target_labeled_this_ta_pair = True
                
                if not target_labeled_this_ta_pair and target_text and target_text != "NULL":
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®ä½“æ˜¯å¦åœ¨contentä¸­ï¼Œä»¥åŠoffset_mapping
                    found_raw = content.find(target_text) != -1
                    print(f"è­¦å‘Š: æ ·æœ¬ID {item_id} (TAå¯¹ {ta_pair_idx}), æœªèƒ½ä¸ºTarget '{target_text}' ç”ŸæˆTargetå±‚tokenæ ‡ç­¾. "
                          f"åœ¨Contentä¸­æ‰¾åˆ°: {found_raw}. Content: '{content[:70]}...'")
                    # if found_raw: # å¦‚æœæ‰¾åˆ°äº†ä½†æ˜ å°„å¤±è´¥ï¼Œæ‰“å°offset
                    #     print(f"Offset Mapping (sample): {offset_mapping[:20]}")


                # --- 2. æ ‡è®°Argumentå±‚ ---
                argument_labeled_this_ta_pair = False
                if argument_text and argument_text != "NULL":
                    chosen_a_char_span = self._find_first_occurrence(content, argument_text)
                    if chosen_a_char_span:
                        a_tok_start, a_tok_end = self._map_char_span_to_token_span_robust(
                            chosen_a_char_span[0], chosen_a_char_span[1], offset_mapping
                        )
                        if a_tok_start != -1:
                            self._apply_iob_labels_to_sequence(
                                a_tok_start, a_tok_end, labels_argument
                            )
                            argument_labeled_this_ta_pair = True
                
                if not argument_labeled_this_ta_pair and argument_text and argument_text != "NULL":
                     found_raw = content.find(argument_text) != -1
                     print(f"è­¦å‘Š: æ ·æœ¬ID {item_id} (TAå¯¹ {ta_pair_idx}), æœªèƒ½ä¸ºArgument '{argument_text}' ç”ŸæˆArgumentå±‚tokenæ ‡ç­¾. "
                           f"åœ¨Contentä¸­æ‰¾åˆ°: {found_raw}. Content: '{content[:70]}...'")
                     # if found_raw:
                     #    print(f"Offset Mapping (sample): {offset_mapping[:20]}")
            
            sample = {
                "input_ids": torch.tensor(input_ids, dtype=torch.long),
                "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
                "labels_target": torch.tensor(labels_target, dtype=torch.long),
                "labels_argument": torch.tensor(labels_argument, dtype=torch.long)
            }
            if token_type_ids is not None and len(token_type_ids) > 0:
                 sample["token_type_ids"] = torch.tensor(token_type_ids, dtype=torch.long)
            processed_samples.append(sample)
        return processed_samples

    def __len__(self):
        return len(self.processed_data)

    def __getitem__(self, idx):
        return self.processed_data[idx]


# --- ç¤ºä¾‹ç”¨æ³• ---
if __name__ == "__main__":
    import json
    import os

    if 'GENERIC_IOB_LABEL_TO_ID' not in globals():
        print("Running ie_dataset.py directly, using inline GENERIC_IOB definitions for testing.")
        GENERIC_IOB_LABELS = ["O", "B", "I"]
        GENERIC_IOB_LABEL_TO_ID = {"O": 0, "B": 1, "I": 2}
        GENERIC_ID_TO_IOB_LABEL = {i: label for i, label in enumerate(GENERIC_IOB_LABELS)}
        IGNORE_INDEX = -100
        HateIEDataset.iob_label_to_id = GENERIC_IOB_LABEL_TO_ID
        HateIEDataset.id_to_iob_label = GENERIC_ID_TO_IOB_LABEL

    temp_original_data = [
        { "id": 3534, "content": "æœ‰ä¸‰ä¸ªè±ªçˆ½çš„åœ°æ–¹å¾ˆç‰¹åˆ«ï¼Œä¸ä¼—ä¸åŒã€‚å¯ä»¥é»‘åˆ«äººï¼Œåˆ«äººä¸èƒ½è¯´å®ƒä»¬ã€‚ğŸ˜†", "output": "å®ƒä»¬ | å¯ä»¥é»‘åˆ«äººï¼Œåˆ«äººä¸èƒ½è¯´å®ƒä»¬ | Region | hate [END]"},
        { "id": 5672, "content": "é™¤éæ˜¯æƒ³å¾·è‰¾æ»‹ç—…ã€‚", "output": "é™¤éæ˜¯æƒ³å¾·è‰¾æ»‹ç—… | é™¤éæ˜¯æƒ³å¾·è‰¾æ»‹ç—… | non-hate | non-hate [END]"},
        { "id": 1, "content": "å¼ ä¸‰æ‰“æå››å› ä¸ºæå››å·ä¸œè¥¿ã€‚", "output": "å¼ ä¸‰ | æ‰“æå›› | LGBTQ | Hateful [SEP] æå›› | å·ä¸œè¥¿ | LGBTQ | Hateful [END]"},
        { "id": 7620, "content": "ç»™ä»–è„¸äº†ï¼Œè¿™ä¸110", "output": "ä»– | ç»™ä»–è„¸äº† | non-hate | non-hate [END]"},
        { "id": 3828, "content": "å¥¹æ²¡èµ„æ ¼è¯„ä»·æˆ‘ï¼Œå¥¹è‡ªå·±ä»€ä¹ˆæ ·å¿ƒé‡Œæ¸…æ¥š", "output": "å¥¹ | æ²¡èµ„æ ¼è¯„ä»·æˆ‘ | LGBTQ | Hateful [SEP] å¥¹è‡ªå·± | ä»€ä¹ˆæ ·å¿ƒé‡Œæ¸…æ¥š | LGBTQ | Hateful [END]"}, # "å¥¹" å‡ºç°å¤šæ¬¡
        { "id": 5208, "content": "åŒæ€§å–å‘ä¸æ˜¯ç—…ï¼ŒæåŒæ‰æ˜¯ç—…ï¼ŒåŒæ€§å–å‘æ˜¯å¤©ç”Ÿçš„ã€‚", "output": "åŒæ€§å–å‘ | ä¸æ˜¯ç—… | LGBTQ | NonHateful [SEP] æåŒ | æ‰æ˜¯ç—… | LGBTQ | Hateful [SEP] åŒæ€§å–å‘ | æ˜¯å¤©ç”Ÿçš„ | LGBTQ | NonHateful [END]"}, # "åŒæ€§å–å‘" å‡ºç°å¤šæ¬¡
        { "id": 6459, "content": "æ²³å—çš„ç”µä¿¡è¯ˆéª—å…¨å›½é—»åï¼Œæ€ä¹ˆæ´—éƒ½æ²¡ç”¨ï¼Œä¸Šæ¬¡å»æ²³å—çš„æ´›é˜³ç©å·®ç‚¹è¢«å·äº†é’±åŒ…ã€‚", "output": "æ²³å—çš„ | ç”µä¿¡è¯ˆéª—å…¨å›½é—»å | Region | Hateful [END]"}
    ]
    temp_json_path = "temp_ie_simplified_filter_test_data.json"
    with open(temp_json_path, 'w', encoding='utf-8') as f:
        json.dump(temp_original_data, f, ensure_ascii=False, indent=2)

    tokenizer_path = "models/chinese-roberta-wwm-ext-large"
    # tokenizer_path = "hfl/chinese-roberta-wwm-ext" 

    print(f"ä½¿ç”¨Tokenizer: {tokenizer_path}")
    print("-" * 30)

    try:
        # ç§»é™¤äº† max_char_overlap_ratio_with_other_ta_pairs å‚æ•°ï¼Œå› ä¸ºå®ƒä¸å†è¢«ä½¿ç”¨
        ie_dataset = HateIEDataset(
            original_json_path=temp_json_path,
            tokenizer_name_or_path=tokenizer_path,
            max_seq_length=128 # å¢åŠ max_seq_lengthä»¥å®¹çº³æ›´é•¿çš„content
        )
    except Exception as e:
        print(f"åˆå§‹åŒ– HateIEDataset å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        if os.path.exists(temp_json_path): os.remove(temp_json_path)
        exit()

    print(f"HateIEDataset å¤§å°: {len(ie_dataset)}")
    print("-" * 30)

    if not ie_dataset:
        print("æœªèƒ½ç”Ÿæˆä»»ä½•å¤„ç†åçš„æ ·æœ¬ã€‚")
    else:
        for i in range(len(ie_dataset)):
            sample = ie_dataset[i]
            original_sample_info = ie_dataset.original_dataset[i] # (id, content, ta_pairs_dicts)
            print(f"\n--- å¤„ç†åæ ·æœ¬ {i} (åŸå§‹ID: {original_sample_info[0]}) ---")
            print(f"åŸå§‹Content: {original_sample_info[1]}")
            print(f"åŸå§‹TAå¯¹: {original_sample_info[2]}")


            original_tokens = ie_dataset.tokenizer.convert_ids_to_tokens(sample['input_ids'].tolist())
            
            readable_labels_tgt = []
            for token_idx, label_id in enumerate(sample['labels_target'].tolist()):
                if label_id != IGNORE_INDEX:
                    readable_labels_tgt.append(f"{original_tokens[token_idx]}({ie_dataset.id_to_iob_label.get(label_id, 'UNK')})")
            print(f"Readable Labels Target: {' '.join(readable_labels_tgt)}")

            readable_labels_arg = []
            for token_idx, label_id in enumerate(sample['labels_argument'].tolist()):
                if label_id != IGNORE_INDEX:
                    readable_labels_arg.append(f"{original_tokens[token_idx]}({ie_dataset.id_to_iob_label.get(label_id, 'UNK')})")
            print(f"Readable Labels Argument: {' '.join(readable_labels_arg)}")
    
    if os.path.exists(temp_json_path): os.remove(temp_json_path)
    print("\nç®€åŒ–è¿‡æ»¤çš„ HateIEDataset æµ‹è¯•è¿è¡Œå®Œæˆã€‚")