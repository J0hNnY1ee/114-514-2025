# information_extractor.py
import torch
from transformers import AutoTokenizer, AutoConfig # éœ€è¦ AutoConfig
from typing import List, Dict, Optional, Tuple, Union
import os

try:
    from models.ie_model import RobertaForMultiLabelTokenClassification 
    from data_process.preprocessor import GENERIC_ID_TO_IOB_LABEL, GENERIC_IOB_LABELS, IGNORE_INDEX
except ImportError as e:
    print(f"ImportError in InformationExtractor: {e}. ç¡®ä¿ model.py å’Œ preprocessor.py å¯è®¿é—®ã€‚")
    GENERIC_ID_TO_IOB_LABEL = {0: "O", 1: "B", 2: "I"}
    GENERIC_IOB_LABELS = ["O", "B", "I"]
    IGNORE_INDEX = -100
    class RobertaForMultiLabelTokenClassification: # Placeholder
        def __init__(self, model_name_or_path, num_single_layer_labels, dropout_prob=0.1): pass
        def load_state_dict(self, state_dict): pass
        def to(self, device): pass
        def eval(self): pass


class InformationExtractor:
    def __init__(
        self,
        trained_model_checkpoint_path: str,  # è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡pytorch_model.binæ‰€åœ¨çš„ç›®å½•
        base_model_name_or_path: str,       # ç”¨äºåŠ è½½configå’Œtokenizerçš„åŸºç¡€æ¨¡å‹è·¯å¾„/åç§°
        device: Union[str, torch.device] = "cpu",
        max_seq_length: int = 512,
        id_to_iob_label_map: Optional[Dict[int, str]] = None,
        num_iob_labels_per_layer: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–ä¿¡æ¯æŠ½å–å™¨ã€‚

        å‚æ•°:
            trained_model_checkpoint_path (str): åŒ…å«è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ (pytorch_model.bin) çš„ç›®å½•è·¯å¾„ã€‚
            base_model_name_or_path (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ 'models/chinese-roberta-wwm-ext-large' æˆ– 
                                           Hugging Face Hubä¸Šçš„åç§°ï¼‰çš„è·¯å¾„æˆ–åç§°ã€‚
                                           å°†ä»æ­¤è·¯å¾„åŠ è½½ AutoConfig å’Œ AutoTokenizerã€‚
            device (Union[str, torch.device]): "cpu" æˆ– "cuda" æˆ– torch.device å¯¹è±¡ã€‚
            max_seq_length (int): è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚
            id_to_iob_label_map (Dict[int, str], optional): é€šç”¨IOBæ ‡ç­¾IDåˆ°æ ‡ç­¾å­—ç¬¦ä¸²çš„æ˜ å°„ã€‚
            num_iob_labels_per_layer (int, optional): æ¯ä¸ªç‹¬ç«‹æ ‡æ³¨å±‚çš„IOBæ ‡ç­¾æ•°é‡ (é€šå¸¸æ˜¯3: O,B,I)ã€‚
        """
        if isinstance(device, str):
            self.device = torch.device(device)
        else:
            self.device = device

        # Tokenizer å’Œ Config ä» base_model_name_or_path åŠ è½½
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)
        self.max_seq_length = max_seq_length

        self.id_to_iob_label = id_to_iob_label_map if id_to_iob_label_map else GENERIC_ID_TO_IOB_LABEL
        self.num_iob_labels = num_iob_labels_per_layer if num_iob_labels_per_layer else len(GENERIC_IOB_LABELS)

        try:
            # 1. å®ä¾‹åŒ–æ¨¡å‹ç»“æ„ï¼Œä½¿ç”¨ base_model_name_or_path æ¥è·å–é…ç½®
            # RobertaForMultiLabelTokenClassification çš„ model_name_or_path å‚æ•°ç”¨äºåŠ è½½å…¶å†…éƒ¨çš„ AutoModel å’Œ AutoConfig
            self.model = RobertaForMultiLabelTokenClassification(
                model_name_or_path=base_model_name_or_path, # ç”¨äºå†…éƒ¨çš„ AutoModel å’Œ AutoConfig
                num_single_layer_labels=self.num_iob_labels
            )
            
            # 2. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
            model_weights_path = os.path.join(trained_model_checkpoint_path, "pytorch_model.bin")
            if not os.path.exists(model_weights_path):
                raise FileNotFoundError(
                    f"è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ 'pytorch_model.bin' æœªåœ¨ '{trained_model_checkpoint_path}' æ‰¾åˆ°ã€‚"
                )
            
            print(f"å°è¯•ä» '{model_weights_path}' åŠ è½½æƒé‡...")
            self.model.load_state_dict(
                torch.load(model_weights_path, map_location=self.device)
            )
            
            self.model.to(self.device)
            self.model.eval()
            print(f"å¤šæ ‡ç­¾ä¿¡æ¯æŠ½å–æ¨¡å‹å·²ä» '{trained_model_checkpoint_path}' (æƒé‡) "
                  f"å’Œ '{base_model_name_or_path}' (é…ç½®/åŸºç¡€æ¨¡å‹) åŠ è½½åˆ° {self.device}")
        except Exception as e:
            print(f"ä» '{trained_model_checkpoint_path}' åŠ è½½IEæ¨¡å‹æ—¶å‡ºé”™: {e}")
            print("è¯·ç¡®ä¿ï¼š")
            print(f"  1. '{base_model_name_or_path}' æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„åŸºç¡€æ¨¡å‹è·¯å¾„/åç§°ï¼ŒåŒ…å«config.jsonã€‚")
            print(f"  2. '{trained_model_checkpoint_path}' ç›®å½•ä¸­åŒ…å« 'pytorch_model.bin' æ–‡ä»¶ã€‚")
            print(f"  3. æ¨¡å‹ç»“æ„ (RobertaForMultiLabelTokenClassification) ä¸ä¿å­˜çš„æƒé‡åŒ¹é…ã€‚")
            raise

    # _decode_iob_to_typed_entities æ–¹æ³•ä¿æŒä¸å˜
    def _decode_iob_to_typed_entities(
        self, tokens: List[str], iob_tags: List[str], entity_main_type: str
    ) -> List[Dict[str, str]]:
        entities = []
        current_entity_tokens = []
        for i, tag in enumerate(iob_tags):
            token = tokens[i]
            if tag == "B":
                if current_entity_tokens:
                    entities.append({"entity_type": entity_main_type, "text": "".join(current_entity_tokens)})
                current_entity_tokens = [token]
            elif tag == "I":
                if current_entity_tokens:
                    current_entity_tokens.append(token)
                else: # I without B
                    current_entity_tokens = [token] # Treat as B
            elif tag == "O":
                if current_entity_tokens:
                    entities.append({"entity_type": entity_main_type, "text": "".join(current_entity_tokens)})
                current_entity_tokens = []
        if current_entity_tokens:
            entities.append({"entity_type": entity_main_type, "text": "".join(current_entity_tokens)})
        return entities

    # _pair_targets_and_arguments_advanced æ–¹æ³•ä¿æŒä¸å˜
    def _pair_targets_and_arguments_advanced(
        self, 
        target_entities: List[Dict[str, str]], 
        argument_entities: List[Dict[str, str]],
        original_text: str 
    ) -> List[Dict[str, str]]:
        if not target_entities and not argument_entities: return []
        if not target_entities:
            return [{"target": "NULL", "argument": arg_e["text"]} for arg_e in argument_entities]
        if not argument_entities:
            return [{"target": tgt_e["text"], "argument": "NULL"} for tgt_e in target_entities]

        def get_char_spans(text, entity_list):
            spans = []
            # ç®€å•åœ°ä¸ºæ¯ä¸ªè§£ç å‡ºçš„å®ä½“æ–‡æœ¬åœ¨åŸæ–‡ä¸­æŸ¥æ‰¾ç¬¬ä¸€æ¬¡å‡ºç°
            # æ³¨æ„ï¼šå¦‚æœå®ä½“æ–‡æœ¬åœ¨åŸæ–‡ä¸­å¤šæ¬¡å‡ºç°ï¼Œè¿™å¯èƒ½ä¸å‡†ç¡®åœ°åæ˜ åŸå§‹TAå¯¹çš„ä½ç½®
            for entity in entity_list:
                entity_text_to_find = entity.get("text", "")
                if not entity_text_to_find: # è·³è¿‡ç©ºå®ä½“æ–‡æœ¬
                    spans.append({"text": "", "start": -1, "end": -1, "original_entity": entity})
                    continue
                try:
                    start = text.find(entity_text_to_find) 
                    if start != -1:
                        spans.append({"text": entity_text_to_find, "start": start, "end": start + len(entity_text_to_find), "original_entity": entity})
                    else:
                        spans.append({"text": entity_text_to_find, "start": -1, "end": -1, "original_entity": entity})
                except TypeError:
                     spans.append({"text": str(entity_text_to_find), "start": -1, "end": -1, "original_entity": entity})
            spans.sort(key=lambda x: x["start"] if x["start"] != -1 else float('inf'))
            return spans

        target_spans_with_info = get_char_spans(original_text, target_entities)
        argument_spans_with_info = get_char_spans(original_text, argument_entities)
        
        pairs = []
        used_arg_indices = [False] * len(argument_spans_with_info)

        for t_info in target_spans_with_info:
            current_target_text = t_info["text"]
            if t_info["start"] == -1 and current_target_text != "NULL": # å¦‚æœTargetæ–‡æœ¬æœ‰æ•ˆä½†æ‰¾ä¸åˆ°ä½ç½®
                print(f"è­¦å‘Š: åœ¨é…å¯¹æ—¶ï¼ŒTargetå®ä½“ '{current_target_text}' æœªåœ¨åŸæ–‡ '{original_text[:50]}...' ä¸­æ‰¾åˆ°ã€‚")
                # ä»ç„¶å°è¯•ä¸ºå®ƒé…å¯¹ï¼Œæˆ–è€…ç›´æ¥ç»™å®ƒä¸€ä¸ªNULL argument
                # pairs.append({"target": current_target_text, "argument": "NULL"})
                # continue

            best_arg_text_match = "NULL" # é»˜è®¤ä¸ºNULL
            best_arg_idx = -1
            # ä½¿ç”¨ä¸€ä¸ªåˆ†æ•°æ¥é€‰æ‹©æœ€ä½³Argumentï¼Œä¾‹å¦‚é‡å åº¦ä¼˜å…ˆï¼Œå…¶æ¬¡æ˜¯é‚»è¿‘åº¦
            best_match_score = -1 

            for j, a_info in enumerate(argument_spans_with_info):
                if not used_arg_indices[j] and a_info["start"] != -1:
                    current_arg_text = a_info["text"]
                    score = 0
                    # è®¡ç®—Targetå’ŒArgumentä¹‹é—´çš„å…³ç³»åˆ†æ•°
                    # 1. é‡å åº¦
                    if t_info["start"] != -1: # åªæœ‰å½“Targetä½ç½®å·²çŸ¥æ—¶æ‰èƒ½è®¡ç®—é‡å 
                        overlap_start = max(t_info["start"], a_info["start"])
                        overlap_end = min(t_info["end"], a_info["end"])
                        overlap_length = max(0, overlap_end - overlap_start)
                        if overlap_length > 0:
                            score += 100 + overlap_length # é‡å æ˜¯å¼ºä¿¡å·
                    
                    # 2. é‚»è¿‘åº¦ (Argumentåœ¨Targetä¹‹å)
                    if t_info["start"] != -1 and a_info["start"] >= t_info["end"]: # Argumentåœ¨Targetä¹‹å
                        distance = a_info["start"] - t_info["end"]
                        score += max(0, 50 - distance) # è¶Šè¿‘åˆ†æ•°è¶Šé«˜
                    elif t_info["start"] != -1 and a_info["end"] <= t_info["start"]: # Argumentåœ¨Targetä¹‹å‰
                        distance = t_info["start"] - a_info["end"]
                        score += max(0, 20 - distance) # ä¹Ÿç»™ä¸€ç‚¹åˆ†ï¼Œä½†ä½äºåœ¨åçš„

                    if score > best_match_score:
                        best_match_score = score
                        best_arg_text_match = current_arg_text
                        best_arg_idx = j
            
            pairs.append({"target": current_target_text, "argument": best_arg_text_match})
            if best_arg_idx != -1:
                used_arg_indices[best_arg_idx] = True
        
        # æ·»åŠ é‚£äº›æ²¡æœ‰é…å¯¹ä¸Šçš„Argument
        for j, a_info in enumerate(argument_spans_with_info):
            if not used_arg_indices[j] and a_info["start"] != -1:
                pairs.append({"target": "NULL", "argument": a_info["text"]})
        
        valid_pairs = [p for p in pairs if not (p["target"] == "NULL" and p["argument"] == "NULL")]
        if not valid_pairs and (target_entities or argument_entities): # å¦‚æœæœ‰å®ä½“ä½†æ²¡é…ä¸Šå¯¹
             # å¯èƒ½æ‰€æœ‰Téƒ½é…äº†NULL Aï¼Œæ‰€æœ‰Aéƒ½é…äº†NULL Tï¼Œç„¶åè¢«è¿‡æ»¤äº†
             # è‡³å°‘è¿”å›ä¸€ä¸ªï¼Œé¿å…å®Œå…¨ç©ºï¼Œé™¤éTå’ŒAåˆ—è¡¨éƒ½ä¸ºç©º
            if target_entities and not argument_entities:
                return [{"target": t["text"], "argument": "NULL"} for t in target_entities]
            if argument_entities and not target_entities:
                return [{"target": "NULL", "argument": a["text"]} for a in argument_entities]

        return valid_pairs

    # extract æ–¹æ³•ä¿æŒä¸å˜ï¼Œå®ƒè°ƒç”¨çš„æ˜¯ä¿®æ”¹åçš„_decodeå’Œ_pairæ–¹æ³•
    def extract(
        self, text: Union[str, List[str]]
    ) -> Union[List[Dict[str, str]], List[List[Dict[str, str]]]]:
        is_single_input = isinstance(text, str)
        texts_to_process = [text] if is_single_input else text
        all_results = []

        for single_text in texts_to_process:
            inputs = self.tokenizer(
                single_text, max_length=self.max_seq_length, padding="max_length",
                truncation=True, return_offsets_mapping=True,
                return_attention_mask=True, return_tensors="pt"
            )
            input_ids_tensor = inputs["input_ids"].to(self.device)
            attention_mask_tensor = inputs["attention_mask"].to(self.device)
            token_type_ids_tensor = inputs.get("token_type_ids")
            if token_type_ids_tensor is not None:
                token_type_ids_tensor = token_type_ids_tensor.to(self.device)

            with torch.no_grad():
                model_outputs = self.model(
                    input_ids=input_ids_tensor,
                    attention_mask=attention_mask_tensor,
                    token_type_ids=token_type_ids_tensor,
                    return_dict=True
                )
                logits_target = model_outputs["logits_target"]
                logits_argument = model_outputs["logits_argument"]
            
            preds_ids_target = torch.argmax(logits_target, dim=-1).squeeze(0).cpu().tolist()
            preds_ids_argument = torch.argmax(logits_argument, dim=-1).squeeze(0).cpu().tolist()

            tokens_for_decoding = []
            iob_tags_target_str = []
            iob_tags_argument_str = []
            
            offsets = inputs["offset_mapping"].squeeze(0).cpu().tolist()
            attention_mask_list = attention_mask_tensor.squeeze(0).cpu().tolist()

            for i in range(len(preds_ids_target)):
                if attention_mask_list[i] == 1:
                    start_char, end_char = offsets[i]
                    if start_char == end_char == 0: continue
                    
                    token_id = input_ids_tensor[0, i].item()
                    if token_id in self.tokenizer.all_special_ids:
                        continue
                    
                    tokens_for_decoding.append(single_text[start_char:end_char])
                    iob_tags_target_str.append(self.id_to_iob_label.get(preds_ids_target[i], "O"))
                    iob_tags_argument_str.append(self.id_to_iob_label.get(preds_ids_argument[i], "O"))
            
            target_entities = self._decode_iob_to_typed_entities(tokens_for_decoding, iob_tags_target_str, "TGT")
            argument_entities = self._decode_iob_to_typed_entities(tokens_for_decoding, iob_tags_argument_str, "ARG")
            
            ta_pairs = self._pair_targets_and_arguments_advanced(target_entities, argument_entities, single_text)
            all_results.append(ta_pairs)

        return all_results[0] if is_single_input else all_results


# --- ç¤ºä¾‹ç”¨æ³• ---
if __name__ == "__main__":
    # æŒ‡å‘ä½ è®­ç»ƒå¥½çš„å¤šæ ‡ç­¾IEæ¨¡å‹çš„æƒé‡æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
    TRAINED_MODEL_CHECKPOINT_DIR = "models/outputs/ie_multilabel/best_model_on_eval_loss" 
    # æŒ‡å‘ç”¨äºè·å–configå’Œtokenizerçš„åŸºç¡€æ¨¡å‹
    BASE_MODEL_FOR_CONFIG_AND_TOKENIZER = "models/chinese-roberta-wwm-ext-large"
    # BASE_MODEL_FOR_CONFIG_AND_TOKENIZER = "hfl/chinese-roberta-wwm-ext" # å¤‡ç”¨

    if 'GENERIC_ID_TO_IOB_LABEL' not in globals(): # ç¡®ä¿æµ‹è¯•æ—¶å®šä¹‰äº†
        print("Running information_extractor.py directly, using inline GENERIC_IOB definitions.")
        GENERIC_IOB_LABELS = ["O", "B", "I"]
        GENERIC_ID_TO_IOB_LABEL = {0:"O", 1:"B", 2:"I"}

    if not os.path.exists(os.path.join(TRAINED_MODEL_CHECKPOINT_DIR, "pytorch_model.bin")):
        print(f"é”™è¯¯: è®­ç»ƒå¥½çš„å¤šæ ‡ç­¾IEæ¨¡å‹æƒé‡ 'pytorch_model.bin' æœªåœ¨ '{TRAINED_MODEL_CHECKPOINT_DIR}' æ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒã€‚")
    else:
        try:
            extractor = InformationExtractor(
                trained_model_checkpoint_path=TRAINED_MODEL_CHECKPOINT_DIR,
                base_model_name_or_path=BASE_MODEL_FOR_CONFIG_AND_TOKENIZER,
                device="cuda" if torch.cuda.is_available() else "cpu",
                id_to_iob_label_map=GENERIC_ID_TO_IOB_LABEL, # ä½¿ç”¨é€šç”¨æ˜ å°„
                num_iob_labels_per_layer=len(GENERIC_IOB_LABELS)
            )

            test_texts = [
                "æœ‰ä¸‰ä¸ªè±ªçˆ½çš„åœ°æ–¹å¾ˆç‰¹åˆ«ï¼Œä¸ä¼—ä¸åŒã€‚å¯ä»¥é»‘åˆ«äººï¼Œåˆ«äººä¸èƒ½è¯´å®ƒä»¬ã€‚ğŸ˜†", 
                "é™¤éæ˜¯æƒ³å¾·è‰¾æ»‹ç—…ã€‚", 
                "å¼ ä¸‰æ‰“æå››å› ä¸ºæå››å·ä¸œè¥¿ã€‚", 
                "ç»™ä»–è„¸äº†ï¼Œè¿™ä¸110", 
                "è¿™éƒ¨ç”µå½±çœŸå¥½çœ‹ï¼Œä¸»è§’æ¼”çš„ä¹Ÿå¥½ï¼Œé…è§’æ¼”çš„ä¹Ÿå¥½ã€‚" # T1=ç”µå½±, A1=å¥½çœ‹; T2=ä¸»è§’, A2=æ¼”çš„å¥½; T3=é…è§’, A3=æ¼”çš„å¥½
            ]

            print(f"\n--- å¼€å§‹ä½¿ç”¨å¤šæ ‡ç­¾æ¨¡å‹æŠ½å– Target å’Œ Argument (æ–°åˆå§‹åŒ–æ–¹å¼) ---")
            for i, text in enumerate(test_texts):
                print(f"\nåŸå§‹æ–‡æœ¬ {i+1}: {text}")
                extracted_pairs = extractor.extract(text)
                if extracted_pairs:
                    for pair_idx, pair in enumerate(extracted_pairs):
                        print(f"  æŠ½å–å¯¹ {pair_idx+1}: Target='{pair['target']}', Argument='{pair['argument']}'")
                else:
                    print("  (æœªæŠ½å–åˆ° Target-Argument å¯¹)")
        except Exception as e:
            print(f"è¿è¡Œç¤ºä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()